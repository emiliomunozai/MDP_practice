{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_1. Formalize the above instance of the Lake MDP mathematically, i.e., explicitly give the set of states, the set of actions, the probability distribution, and the reward function._\n",
    "\n",
    "*State space (𝑆):*\n",
    "\n",
    "The environment is 4×4 grid.\n",
    "The agent can occupy any non-hole, non-wall cell.\n",
    "There are special states:\n",
    "    Goal state (bottom-right corner).\n",
    "    Hole states (absorbing states with a large negative reward).\n",
    "\n",
    "Therefore, 𝑆\n",
    "S consists of all possible positions (𝑖,𝑗) in the grid.\n",
    "\n",
    "    s ∈ [{1,1},{1,2},{1,3},{1,4},{2,1},{2,2},{2,3},{2,4},….{4,4}]\n",
    "\n",
    "*Action space (A):*\n",
    "\n",
    "    a ∈ [up, righ, down, left]\n",
    "    a ∈ A(s) Since there are some state that dont have all actions available\n",
    "\n",
    "*Transition probabilities(P):*\n",
    "\n",
    "    P = p(s' | s, a)\n",
    "    Where moving in an intended direction have 0.8 chance of succes\n",
    "    If adjacents cells are unblocked there is a probabilty of 0.2 to arrive into a different state. Equally distributed.\n",
    "    If one is blocked the probablity would be 0.1.\n",
    "    If both are blocked the probablity would be 0.2 of not moving.\n",
    "\n",
    "*Reward:*\n",
    "\n",
    "    R(s,a) = -0.1 for all non terminal states\n",
    "    R(hole) = -1,000 for holes\n",
    "    R(goal) = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_2. Does there exist a policy for the above instance of the Lake MDP that can surely arrive at the goal without running the risk of falling into a hole? Explain_\n",
    "\n",
    "In the current example the agent always risk falling into a hole since there is no route that it could take that does not have a a hole adjacent to the floor tile.\n",
    "\n",
    "_3. Is a policy that keeps the agent on the lake forever reasonable? How would a reasonable policy for the above instances of the Lake MDP look like? Write it down explicitly._\n",
    "\n",
    "No, since each state have a reward of -1 if it stays indefinetly the reward grows and even becomes worst than falling into a hole. \n",
    "\n",
    "Down, down, right, down, right, right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python modules autoreload setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.core.base_mdp import MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial States: [(0, 0)]\n",
      "All States: [(0, 0), (0, 1), (0, 2), (0, 3), (1, 0), (1, 2), (2, 0), (2, 1), (2, 2), (3, 1), (3, 2), (3, 3)]\n",
      "Actions in (2,1): ['DOWN', 'LEFT', 'RIGHT']\n",
      "Reward for (3,3): 10\n",
      "Transition from (0, 1) taking 'DOWN': {(2, 2): 0.8, (2, 1): 0.1, (3, 1): 0.1}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LakeMDP(MDP):\n",
    "    def __init__(self, lake_grid: np.ndarray):\n",
    "        \"\"\"\n",
    "        Initializes the LakeMDP with a given binary grid.\n",
    "        \n",
    "        :param lake_grid: A numpy array where 0 represents viable fields and 1 represents holes.\n",
    "        \"\"\"\n",
    "        assert isinstance(lake_grid, np.ndarray), \"lake_grid must be a NumPy array\"\n",
    "        assert lake_grid.dtype == np.int_, \"lake_grid must be a binary integer array\"\n",
    "        assert lake_grid.shape[0] > 1 and lake_grid.shape[1] > 1, \"lake_grid must be at least 2x2\"\n",
    "        \n",
    "        # Enforce constraints: start (0,0) and goal (-1,-1) must be viable\n",
    "        lake_grid[0, 0] = 0  \n",
    "        lake_grid[-1, -1] = 0  \n",
    "\n",
    "        self.lake_grid = lake_grid\n",
    "        self.n_rows, self.n_cols = lake_grid.shape\n",
    "        self.goal_state = (self.n_rows - 1, self.n_cols - 1)\n",
    "\n",
    "        # Define possible actions (up, down, left, right)\n",
    "        self._actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "\n",
    "    @property\n",
    "    def init_states(self) -> list:\n",
    "        \"\"\"Returns the starting position (0,0) as the only initial state.\"\"\"\n",
    "        return [(0, 0)]\n",
    "\n",
    "    @property\n",
    "    def states(self) -> list:\n",
    "        \"\"\"Returns all valid states (where grid value is 0).\"\"\"\n",
    "        return [(r, c) for r in range(self.n_rows) for c in range(self.n_cols) if self.lake_grid[r, c] == 0]\n",
    "\n",
    "    def get_actions_in_state(self, s) -> list:\n",
    "        \"\"\"Returns valid actions in state `s`, ensuring no moves go out of bounds or into holes.\"\"\"\n",
    "        r, c = s\n",
    "        actions = []\n",
    "        \n",
    "        if r > 0 and self.lake_grid[r - 1, c] == 0:\n",
    "            actions.append('UP')\n",
    "        if r < self.n_rows - 1 and self.lake_grid[r + 1, c] == 0:\n",
    "            actions.append('DOWN')\n",
    "        if c > 0 and self.lake_grid[r, c - 1] == 0:\n",
    "            actions.append('LEFT')\n",
    "        if c < self.n_cols - 1 and self.lake_grid[r, c + 1] == 0:\n",
    "            actions.append('RIGHT')\n",
    "\n",
    "        return actions\n",
    "\n",
    "    def get_reward(self, s) -> float:\n",
    "        \"\"\"Returns reward: -1 for normal states, 0 for holes, +10 for goal.\"\"\"\n",
    "        if s == self.goal_state:\n",
    "            return 10  # Reward for reaching goal\n",
    "        elif self.lake_grid[s] == 1:\n",
    "            return 0  # No reward for falling into a hole\n",
    "        return -1  # Default step penalty\n",
    "\n",
    "    def get_transition_distribution(self, s, a) -> dict:\n",
    "        \"\"\"Returns the next state distribution with stochastic transitions.\"\"\"\n",
    "        if s == self.goal_state:\n",
    "            return {s: 1.0}  # If in goal state, stay there\n",
    "\n",
    "        r, c = s\n",
    "        main_move = s  # Default to staying in place\n",
    "\n",
    "        # Define movement directions\n",
    "        moves = {\n",
    "            'UP': (-1, 0),\n",
    "            'DOWN': (1, 0),\n",
    "            'LEFT': (0, -1),\n",
    "            'RIGHT': (0, 1)\n",
    "        }\n",
    "        perpendicular_moves = {\n",
    "            'UP': [('LEFT', (0, -1)), ('RIGHT', (0, 1))],\n",
    "            'DOWN': [('LEFT', (0, -1)), ('RIGHT', (0, 1))],\n",
    "            'LEFT': [('UP', (-1, 0)), ('DOWN', (1, 0))],\n",
    "            'RIGHT': [('UP', (-1, 0)), ('DOWN', (1, 0))]\n",
    "        }\n",
    "\n",
    "        # Compute primary move\n",
    "        if a in moves:\n",
    "            dr, dc = moves[a]\n",
    "            new_r, new_c = r + dr, c + dc\n",
    "            if 0 <= new_r < self.n_rows and 0 <= new_c < self.n_cols and self.lake_grid[new_r, new_c] == 0:\n",
    "                main_move = (new_r, new_c)\n",
    "\n",
    "        # Compute stochastic perpendicular moves\n",
    "        transitions = {main_move: 0.8}  # 80% chance to move as intended\n",
    "        for _, (dr, dc) in perpendicular_moves[a]:\n",
    "            new_r, new_c = r + dr, c + dc\n",
    "            if 0 <= new_r < self.n_rows and 0 <= new_c < self.n_cols and self.lake_grid[new_r, new_c] == 0:\n",
    "                transitions[(new_r, new_c)] = 0.1  # 10% chance to move sideways\n",
    "            else:\n",
    "                transitions[s] = transitions.get(s, 0) + 0.1  # Stay in place if move blocked\n",
    "\n",
    "        return transitions\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"LakeMDP({self.n_rows}x{self.n_cols})\"\n",
    "\n",
    "\n",
    "# Usage\n",
    "lake = np.array([[0, 0, 0, 0],\n",
    "                 [0, 1, 0, 1],\n",
    "                 [0, 0, 0, 1],\n",
    "                 [1, 0, 0, 0]])\n",
    "\n",
    "lake_mdp = LakeMDP(lake)\n",
    "\n",
    "print(\"Initial States:\", lake_mdp.init_states)\n",
    "print(\"All States:\", lake_mdp.states)\n",
    "print(\"Actions in (2,1):\", lake_mdp.get_actions_in_state((2,1)))\n",
    "print(\"Reward for (3,3):\", lake_mdp.get_reward((3,3)))\n",
    "print(\"Transition from (0, 1) taking 'DOWN':\", lake_mdp.get_transition_distribution((2,1), 'RIGHT'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
